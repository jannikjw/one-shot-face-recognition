{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwXdV7BHeoT"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "qaGV91DX1SIf",
        "outputId": "41e58374-8636-4077-c5b5-15867d881dfa"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, SequentialSampler\n",
        "from torchvision import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from src.utils.triplet_loss import BatchAllTtripletLoss\n",
        "from tqdm.notebook import tqdm\n",
        "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, save_file_names, CelebADatasetTriplet, get_train_files_for_max_img_per_person\n",
        "from src.utils.loss_functions import TripletLoss\n",
        "from src.utils.similarity_functions import euclidean_distance_matrix\n",
        "from importlib import reload\n",
        "\n",
        "workers = 0 if os.name == 'nt' else 2\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Ta2e8wa3UM"
      },
      "source": [
        "# Define CelebA Dataset and Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training and testing dataframes\n",
        "\n",
        "mapping_file = 'data/identity_CelebA.txt'\n",
        "\n",
        "file_label_mapping = pd.read_csv(\n",
        "    mapping_file, header=None, sep=\" \", names=[\"file_name\", \"person_id\", \"is_train\"]\n",
        ")\n",
        "\n",
        "train_files = get_train_files_for_max_img_per_person(file_label_mapping=file_label_mapping, max_img_pp=5)\n",
        "\n",
        "file_label_mapping.loc[:, 'is_train'] = 0\n",
        "file_label_mapping.loc[file_label_mapping['file_name'].isin(train_files), 'is_train'] = 1\n",
        "file_label_mapping['file_id'] = [int(elem[:6])-1 for elem in file_label_mapping['file_name'].values]\n",
        "\n",
        "train_df = file_label_mapping[file_label_mapping['is_train']==1]\n",
        "test_df = file_label_mapping[file_label_mapping['is_train']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hkdQB_ZODfmw"
      },
      "outputs": [],
      "source": [
        "## Load the dataset\n",
        "# Path to directory with all the images\n",
        "img_folder = 'data/img_align_celeba'\n",
        "mapping_file = 'data/identity_CelebA.txt'\n",
        "\n",
        "image_size = 160\n",
        "transform=transforms.Compose([ #TODO: Add standardization\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADataset(img_folder, mapping_file, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dP2V5_HTDaMt"
      },
      "outputs": [],
      "source": [
        "## Create a dataloader\n",
        "# Batch size during training\n",
        "batch_size = 8\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "\n",
        "celeba_dataloader = DataLoader(celeba_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=num_workers,\n",
        "                                pin_memory=pin_memory,\n",
        "                                shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_inds = train_df['file_id'].values.tolist()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    celeba_dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(train_inds)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceNet Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the resnet model, optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').to(device)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.0001)\n",
        "criterion = BatchAllTtripletLoss()\n",
        "eps = 1e-8 # constant to ensure no division by 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_positive_observations(X, y, df):\n",
        "    \"\"\"Find the positive observations in the supplied dataset for each observation in X \n",
        "    and adds features and labels to X and y, respectively.\n",
        "\n",
        "    Args:\n",
        "        X (tensor): Features of images. Shape: [batch_size, channels, width, height]\n",
        "        y (tensor): Labels: Shape: [batch_size]\n",
        "        df (pd.DataFrame): Dataframe that contains mapping of file IDs and labels ('person_id')\n",
        "\n",
        "    Returns:\n",
        "        (tensor, tensor): _description_\n",
        "    \"\"\"\n",
        "    \n",
        "    pos_obs_idx = np.array([], dtype=int)\n",
        "    for anchor in np.unique(y):\n",
        "        # get file_ids of all positive examples for anchor\n",
        "        pos_obs_idx = np.hstack([pos_obs_idx, df[df['person_id']==int(anchor)]['file_id'].values])\n",
        "\n",
        "    for pos_obs in pos_obs_idx:\n",
        "        # get image and label of positive example\n",
        "        pos_img, pos_label = celeba_dataset[pos_obs]\n",
        "        # add to batch\n",
        "        X = torch.cat((X, torch.unsqueeze(pos_img, 0)), dim=0)\n",
        "        y = torch.cat((y, torch.tensor([pos_label])), dim=0)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fc4d944f9c24cda9d73fb1efe6c8c6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|                                             | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb4e67b5da174742ada3727a389859ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Current Batch:   0%|                                   | 0/6129 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.3795, grad_fn=<DivBackward0>)\n",
            "tensor(0.4961, grad_fn=<DivBackward0>)\n",
            "tensor(0.4705, grad_fn=<DivBackward0>)\n",
            "tensor(0.4780, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "resnet.train()\n",
        "epochs = 5\n",
        "loss_total = []\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True, ncols=80, position=0):\n",
        "    running_loss = []\n",
        "    for idx, (X, y) in enumerate(tqdm(train_loader, desc=\"Current Batch\", ncols=80, position=1, leave=False)):\n",
        "        X, y = find_positive_observations(X, y, train_df)\n",
        "        \n",
        "        # Create embeddings\n",
        "        X_emb = resnet(X.to(device))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(X_emb, y.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "        \n",
        "    loss_total.append(np.mean(running_loss))\n",
        "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch, epochs, np.mean(running_loss)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# printing loss function\n",
        "plt.plot(loss_total)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"TripletLoss\")\n",
        "plt.title(\"Training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vault_path = \"data/oneshot_vault\"\n",
        "label_file = \"data/identity_vault_person.txt\"\n",
        "\n",
        "def load_image(path, transform):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if transform:\n",
        "            img = transform(img)\n",
        "    return img\n",
        "    \n",
        "def create_embeddings(folder, label_file, model, transform):\n",
        "    label_file_dict = {}\n",
        "    gt_labels = []\n",
        "    with open(label_file, 'r') as r_file:\n",
        "        for file in r_file:\n",
        "            file = file.strip(\"\\n\").split(\" \")\n",
        "            if file[0] not in label_file_dict:\n",
        "                label_file_dict[file[0]] = file[1]\n",
        "\n",
        "    embeddings = torch.empty(len(label_file_dict), 512)\n",
        "    for i, file in enumerate(label_file_dict.keys()):\n",
        "        img = load_image(os.path.join(folder, file), transform)\n",
        "\n",
        "        img_emb = model(img[None, :])\n",
        "\n",
        "        embeddings[i] = img_emb\n",
        "        gt_labels.append(label_file_dict[file])\n",
        "\n",
        "    return embeddings, gt_labels\n",
        "\n",
        "\n",
        "resnet.eval().to(device)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "embeddings, gt_labels = create_embeddings(folder=vault_path, label_file=label_file, \n",
        "                        model=resnet, transform=transform)\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test image:\n",
        "def calculate_label(test_image_file, img_folder, transform, embeddings):\n",
        "# test_image_file = \"s1_9.pgm\"\n",
        "    test_file_path = os.path.join(img_folder, test_image_file)\n",
        "    test_img = load_image(test_file_path, transform=transform)\n",
        "\n",
        "\n",
        "    test_img_emb = resnet(test_img[None, :])\n",
        "    test_img_emb = torch.squeeze(test_img_emb, 0)\n",
        "    # print(f'Shape of test: {test_img_emb.shape}')\n",
        "    # print(f'Shape of embeddings: {embeddings.shape}')\n",
        "\n",
        "    distance_mat = (test_img_emb - embeddings).pow(2).sum(axis=1)\n",
        "    # print(distance_mat)\n",
        "    test_label_pred = gt_labels[torch.argmin(distance_mat.abs())]\n",
        "    # print(f'Ground truth label: {test_image_file.split(\"_\")[0][1]}')\n",
        "    # print(f'Predicted label: {test_label_pred}')\n",
        "\n",
        "    return int(test_label_pred)\n",
        "\n",
        "# testing on one test image\n",
        "test_image_file = \"000032.jpg\"\n",
        "calculate_label(test_image_file, img_folder, transform, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open(mapping_file, 'r') as test_labels_file:\n",
        "    test_labels = {}\n",
        "    for i, file in enumerate(test_labels_file):\n",
        "        file = file.strip(\"\\n\").split(\" \")\n",
        "        test_labels[file[0]] = file[1]\n",
        "        # if i > 300:\n",
        "        #     break\n",
        "\n",
        "test_predictions = []\n",
        "test_gt_labels = []\n",
        "\n",
        "for i, (file, label) in enumerate(test_labels.items()):\n",
        "    test_gt_labels.append(int(label))\n",
        "\n",
        "    test_label_pred = calculate_label(file, img_folder, transform, embeddings)\n",
        "    test_predictions.append(test_label_pred)\n",
        "\n",
        "accuracy = torch.tensor(test_predictions) == torch.tensor(test_gt_labels)\n",
        "accuracy = accuracy.int().sum()/len(accuracy)\n",
        "print(f'Accuracy for the model: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9TwXdV7BHeoT",
        "k0mnB3l9Ha9D",
        "w7Ta2e8wa3UM",
        "gU46F-JyF2PA",
        "3JLywKFbopy9",
        "aEBA_FaWHpgY",
        "IJAT0dvwHfaa",
        "vqPiqddKE51v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.15 ('face-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "fa56d487002647ce19700e56c377ef69da6ace515f2aecd558af12c55c24b683"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
