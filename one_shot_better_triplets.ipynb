{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TwXdV7BHeoT"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "qaGV91DX1SIf",
    "outputId": "41e58374-8636-4077-c5b5-15867d881dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization\n",
    "import sklearn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, SequentialSampler\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from src.utils.triplet_loss import BatchAllTripletLoss\n",
    "from tqdm.notebook import tqdm\n",
    "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, CelebADatasetTriplet, get_embeddings_and_file_names\n",
    "from src.utils.loss_functions import TripletLoss\n",
    "from src.utils.similarity_functions import euclidean_distance_matrix\n",
    "from importlib import reload\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "nGPU = torch.cuda.device_count()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ta2e8wa3UM"
   },
   "source": [
    "# Define CelebA Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hkdQB_ZODfmw"
   },
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "# Path to directory with all the images\n",
    "img_folder = 'data/img_align_celeba'\n",
    "mapping_file = 'data/identity_CelebA.txt'\n",
    "\n",
    "image_size = 160\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADataset(img_folder, mapping_file, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET: There are 20266 images in the training data of 10133 people.\n"
     ]
    }
   ],
   "source": [
    "train_img_pp = 2\n",
    "\n",
    "train_files, train_labels = celeba_dataset.create_X_shot_dataset(img_pp=2)\n",
    "print(f\"TRAIN SET: There are {len(train_files)} images in the training data of {len(np.unique(train_labels))} people.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>person_id</th>\n",
       "      <th>is_train</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>2880</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>2937</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>8692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name  person_id  is_train  file_id\n",
       "0  000001.jpg       2880         1        0\n",
       "1  000002.jpg       2937         1        1\n",
       "2  000003.jpg       8692         1        2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flm = celeba_dataset.file_label_mapping\n",
    "\n",
    "flm.loc[:, 'is_train'] = 0\n",
    "flm.loc[flm['file_name'].isin(train_files), 'is_train'] = 1\n",
    "flm['file_id'] = [int(elem[:6])-1 for elem in flm['file_name'].values]\n",
    "\n",
    "train_df = flm[flm['is_train']==1]\n",
    "test_df = flm[flm['is_train']==0]\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positive_observations(self, X, y, df, sample=False, num_examples=5):\n",
    "    \"\"\"Find the positive observations in the supplied dataset for each observation in X \n",
    "    and adds features and labels to X and y, respectively.\n",
    "\n",
    "    Args:\n",
    "        X (tensor): Features of images. Shape: [batch_size, channels, width, height]\n",
    "        y (tensor): Labels: Shape: [batch_size]\n",
    "        df (pd.DataFrame): Dataframe that contains mapping of file IDs and labels ('person_id')\n",
    "\n",
    "    Returns:\n",
    "        (tensor, tensor): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    pos_obs_idx = np.array([], dtype=int)\n",
    "    for anchor in np.unique(y):\n",
    "        # get file_ids of all positive examples for anchor \n",
    "        # positive examples in dataframe\n",
    "        #pos_examples = df[df['person_id']==int(anchor)]['file_id'].values        \n",
    "        pos_examples = df[df['person_id']==int(anchor)].index.values\n",
    "\n",
    "        if sample and len(pos_examples)!=0:\n",
    "            pos_examples = np.random.choice(pos_examples, size=num_examples)\n",
    "\n",
    "        pos_obs_idx = np.hstack([pos_obs_idx, pos_examples])\n",
    "\n",
    "    for pos_obs in pos_obs_idx:\n",
    "        #get image and label of positive example\n",
    "        #pos_img, pos_label = self[pos_obs]\n",
    "        pos_img, pos_label, _ = self[pos_obs]\n",
    "\n",
    "        # add to batch\n",
    "        X = torch.cat((X, torch.unsqueeze(pos_img, 0)), dim=0)\n",
    "        y = torch.cat((y, torch.tensor([pos_label])), dim=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class BatchSampler(Sampler[List[int]]):\n",
    "    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n",
    "    Args:\n",
    "        sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "    Example:\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool) -> None:\n",
    "        # Since collections.abc.Iterable does not check for `__getitem__`, which\n",
    "        # is one way for an object to be an iterable, we don't do an `isinstance`\n",
    "        # check here.\n",
    "        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \\\n",
    "                batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integer value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(\"drop_last should be a boolean value, but got \"\n",
    "                             \"drop_last={}\".format(drop_last))\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        # Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951\n",
    "        if self.drop_last:\n",
    "            sampler_iter = iter(self.sampler)\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = [next(sampler_iter) for _ in range(self.batch_size)]\n",
    "                    yield batch\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        else:\n",
    "            batch = [0] * self.batch_size\n",
    "            idx_in_batch = 0\n",
    "            for idx in self.sampler:\n",
    "                batch[idx_in_batch] = idx\n",
    "                idx_in_batch += 1\n",
    "                if idx_in_batch == self.batch_size:\n",
    "                    yield batch\n",
    "                    idx_in_batch = 0\n",
    "                    batch = [0] * self.batch_size\n",
    "            if idx_in_batch > 0:\n",
    "                yield batch[:idx_in_batch]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Can only be called if self.sampler has __len__ implemented\n",
    "        # We cannot enforce this condition, so we turn off typechecking for the\n",
    "        # implementation below.\n",
    "        # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size  # type: ignore[arg-type]\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size  # type: ignore[arg-type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>person_id</th>\n",
       "      <th>is_train</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>2880</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>2937</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>8692</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>5805</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>9295</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200350</th>\n",
       "      <td>200351.jpg</td>\n",
       "      <td>5269</td>\n",
       "      <td>1</td>\n",
       "      <td>200350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200505</th>\n",
       "      <td>200506.jpg</td>\n",
       "      <td>6768</td>\n",
       "      <td>1</td>\n",
       "      <td>200505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200972</th>\n",
       "      <td>200973.jpg</td>\n",
       "      <td>9392</td>\n",
       "      <td>1</td>\n",
       "      <td>200972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201983</th>\n",
       "      <td>201984.jpg</td>\n",
       "      <td>7942</td>\n",
       "      <td>1</td>\n",
       "      <td>201983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202145</th>\n",
       "      <td>202146.jpg</td>\n",
       "      <td>9169</td>\n",
       "      <td>1</td>\n",
       "      <td>202145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20266 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name  person_id  is_train  file_id\n",
       "0       000001.jpg       2880         1        0\n",
       "1       000002.jpg       2937         1        1\n",
       "2       000003.jpg       8692         1        2\n",
       "3       000004.jpg       5805         1        3\n",
       "4       000005.jpg       9295         1        4\n",
       "...            ...        ...       ...      ...\n",
       "200350  200351.jpg       5269         1   200350\n",
       "200505  200506.jpg       6768         1   200505\n",
       "200972  200973.jpg       9392         1   200972\n",
       "201983  201984.jpg       7942         1   201983\n",
       "202145  202146.jpg       9169         1   202145\n",
       "\n",
       "[20266 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET: There are 20266 images of 10133 people in the train set.\n",
      "TEST SET:  There are 182333 images of 9853 people in the test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/faceenv2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Number of workers for the dataloader\n",
    "num_workers = 12 * nGPU if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "# Train set parameters\n",
    "batch_size = 2\n",
    "\n",
    "is_subset = False\n",
    "size_train_set = 1000\n",
    "\n",
    "# Test set parameters\n",
    "test_batch_size = 256\n",
    "\n",
    "# Sample datasets for faster experimentation\n",
    "subset_people = np.unique(train_df['person_id'])\n",
    "subset_train_df = pd.DataFrame.copy(train_df)\n",
    "subset_test_df = pd.DataFrame.copy(test_df)\n",
    "\n",
    "if is_subset:\n",
    "    subset_people = np.random.choice(np.unique(train_df['person_id']), size=size_train_set, replace=False)\n",
    "    subset_train_df = pd.DataFrame.copy(train_df[train_df['person_id'].isin(subset_people)])\n",
    "    subset_test_df = pd.DataFrame.copy(test_df[test_df['person_id'].isin(subset_people)])\n",
    "\n",
    "train_inds = subset_train_df['file_id'].values.tolist()\n",
    "test_inds = subset_test_df['file_id'].values.tolist()\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    celeba_dataset,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=pin_memory,\n",
    "    sampler=SubsetRandomSampler(train_inds)\n",
    ")\n",
    "    \n",
    "test_loader = DataLoader(\n",
    "    celeba_dataset,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=test_batch_size,\n",
    "    pin_memory=pin_memory,\n",
    "    sampler=SubsetRandomSampler(test_inds)\n",
    ")\n",
    "test_set_num_people = len(np.unique(subset_test_df['person_id']))\n",
    "                                                                   \n",
    "print(f\"TRAIN SET: There are {len(train_inds)} images of {len(np.unique(subset_train_df['person_id']))} people in the train set.\")\n",
    "print(f\"TEST SET:  There are {len(test_inds)} images of {len(np.unique(subset_test_df['person_id']))} people in the test set.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing the resnet model, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.5\n",
    "gamma = 0.1\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "\n",
    "schedule = [40, 80, 130, 160]\n",
    "str_schedule = \"_\".join(map(str, schedule))\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_linear.weight\n",
      "last_bn.weight\n",
      "last_bn.bias\n"
     ]
    }
   ],
   "source": [
    "# @title Freeze upstream layers\n",
    "def set_parameter_requires_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"last\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "set_parameter_requires_grad(resnet)\n",
    "for name, param in resnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define optimizer and loss\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=lr)\n",
    "criterion = BatchAllTripletLoss()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedule, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, y, train_df):\n",
    "    X, y = celeba_dataset.find_positive_observations(X, y, train_df)\n",
    "        \n",
    "    # Create embeddings\n",
    "    X_emb = resnet(X.to(device))\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = criterion(X_emb, y.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train_step\n",
    "X, y, file_names = next(iter(train_loader))\n",
    "train_step(X, y, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.train()\n",
    "epochs = 10\n",
    "loss_total = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
    "    running_loss = []\n",
    "    for step, (X,y) in enumerate(tqdm(train_loader, desc='Current Batch', leave=True)):\n",
    "        loss = train_step(X, y, train_df)\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    loss_total.append(np.mean(running_loss))\n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_str = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "# Save model weights\n",
    "model_state_path = f\"models/facenet_model_statedict_{dt_str}_{size_train_set}people_{train_img_pp}imgpp_{epochs}epochs.pth\"\n",
    "if not os.path.exists(model_state_path):\n",
    "    torch.save(resnet.state_dict(), model_state_path)\n",
    "\n",
    "# Save loss values\n",
    "losses_path = f\"experiments/loss_train_list_{dt_str}_{size_train_set}people_{train_img_pp}imgpp_{epochs}epochs.pth\"\n",
    "if not os.path.exists(model_state_path):\n",
    "    torch.save(torch.FloatTensor(running_loss), losses_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing loss function\n",
    "plt.plot(loss_total)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"TripletLoss\")\n",
    "plt.title(\"Training Triplet Loss\")\n",
    "plt.savefig(f\"loss_curves/loss_curve_{dt_str}_{size_train_set}people_{train_img_pp}imgpp_{epochs}epochs_{lr}lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings - either from files (if exists) or by running model\n",
    "from datetime import datetime\n",
    "\n",
    "train_embeddings_path = f'pytorch_objects/train_embeddings_{dt_str}_{size_train_set}people_{train_img_pp}imgpp_{epochs}epochs.pt'\n",
    "train_labels_path = f'pytorch_objects/train_labels_{dt_str}_{size_train_set}people_{train_img_pp}imgpp_{epochs}epochs.pt'\n",
    "test_embeddings_path = f'pytorch_objects/test_embeddings_{dt_str}_{size_train_set}people_{epochs}epochs.pt'\n",
    "test_labels_path = f'pytorch_objects/test_labels_{dt_str}_{size_train_set}people_{epochs}epochs.pt'\n",
    "\n",
    "train_embeddings, train_labels = get_embeddings_and_file_names(resnet, train_loader, train_embeddings_path, train_labels_path)\n",
    "test_embeddings, test_labels = get_embeddings_and_file_names(resnet, test_loader, test_embeddings_path, test_labels_path)\n",
    "# aug_embeddings, aug_labels = celeba_helper.get_embeddings_and_file_names(resnet, aug_loader, save_tensors=False)\n",
    "    \n",
    "print(f'TRAIN:     Embeddings: {train_embeddings.shape}.\\tLabels: {train_labels.shape}.')\n",
    "# print(f'AUGMENTED: Embeddings: {aug_embeddings.shape}.\\tLabels: {aug_labels.shape}.')\n",
    "print(f'TEST:      Embeddings: {test_embeddings.shape}. Labels: {test_labels.shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train_embeddings, train_labels)\n",
    "score = knn.score(test_embeddings, test_labels)\n",
    "\n",
    "print(f'Pre-trained model: Accuracy = {score}.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9TwXdV7BHeoT",
    "k0mnB3l9Ha9D",
    "w7Ta2e8wa3UM",
    "gU46F-JyF2PA",
    "3JLywKFbopy9",
    "aEBA_FaWHpgY",
    "IJAT0dvwHfaa",
    "vqPiqddKE51v"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "faceenv2",
   "language": "python",
   "name": "faceenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "84f8bbaa9d2768d86c31f828736d209a638bae1d7b7e8ef7ab8a5fdacb37a395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
