{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TwXdV7BHeoT"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qaGV91DX1SIf"
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from src.utils.celeba_dataset import CelebADataset\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SpzbCElm2cqD",
    "outputId": "bcc425d3-1645-460b-ca9f-70a39686d4b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ta2e8wa3UM"
   },
   "source": [
    "# Define CelebA Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkdQB_ZODfmw"
   },
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "# Path to directory with all the images\n",
    "img_folder = 'data/img_align_celeba'\n",
    "mapping_file = 'data/identity_CelebA.txt'\n",
    "\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 160\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADataset(img_folder, mapping_file, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP2V5_HTDaMt"
   },
   "outputs": [],
   "source": [
    "## Create a dataloader\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU46F-JyF2PA"
   },
   "source": [
    "# Setup FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRL58sf7HsUQ"
   },
   "source": [
    "## Define MTCNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "-4xtVzckHtgP"
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=image_size, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, keep_all=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEPP0J3qHxXQ"
   },
   "source": [
    "## Define Inception Resnet V1 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "y1_pYZ6BHz7e"
   },
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEBA_FaWHpgY"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sK8IYp9-Rz0"
   },
   "outputs": [],
   "source": [
    "load_new_data = False\n",
    "file_number_to_load = '028289'\n",
    "num_batches = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDuAgUWtizaI",
    "outputId": "8f9ad910-f08f-4a83-f32e-5ccca73b3f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28246, 512])\n"
     ]
    }
   ],
   "source": [
    "# Custom data loading function\n",
    "\n",
    "def load_data(dataloader: torch.utils.data.DataLoader, num_batches: int):\n",
    "    embeddings = None\n",
    "    face_file_names = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        aligned = torch.tensor([])\n",
    "        train_features, file_names = batch\n",
    "\n",
    "        for train_feature, file_name in zip(train_features, file_names):\n",
    "            img = transforms.ToPILImage()(train_feature)\n",
    "            feature_aligned, prob = mtcnn(img, return_prob=True)\n",
    "            if feature_aligned is not None:\n",
    "                feature_aligned = feature_aligned.reshape([1, 3, image_size, image_size])\n",
    "                face_file_names.append(file_name)\n",
    "                if len(aligned) == 0:\n",
    "                    aligned = feature_aligned\n",
    "                else:\n",
    "                    aligned = torch.cat([aligned, feature_aligned])\n",
    "\n",
    "        print(f'Batch {idx}. Batch shape: {aligned.shape}')\n",
    "        aligned = aligned.to(device)\n",
    "        batch_embeddings = resnet(aligned).detach().cpu()\n",
    "\n",
    "        if embeddings == None:\n",
    "            embeddings = batch_embeddings\n",
    "        else:\n",
    "            embeddings = torch.cat([embeddings, batch_embeddings])\n",
    "\n",
    "        if idx == num_batches:\n",
    "            break\n",
    "\n",
    "    return embeddings, face_file_names\n",
    "\n",
    "if load_new_data:\n",
    "    train_embeddings, train_face_file_names = load_data(celeba_dataloader, num_batches)\n",
    "    torch.save(train_embeddings, f'pytorch_objects/embeddings_up_to_img_{face_file_names[-1][0:-4]}.pickle')\n",
    "    with open(f'pytorch_objects/file_names_up_to_img_{face_file_names[-1][0:-4]}', 'w') as fp:\n",
    "        for item in face_file_names:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "        print('Done')\n",
    "else:\n",
    "    train_embeddings = torch.load(f'pytorch_objects/embeddings_up_to_img_{file_number_to_load}.pickle')\n",
    "    train_face_file_names = []\n",
    "    with open(f'pytorch_objects/file_names_up_to_img_{file_number_to_load}', 'r') as fp:\n",
    "        for line in fp:\n",
    "            x = line[:-1]\n",
    "            # add current item to the list\n",
    "            train_face_file_names.append(x)\n",
    "\n",
    "\n",
    "print(train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wy2VcdcI6Of",
    "outputId": "b83c57ee-1f02-44f4-dbae-3fcb3253ea2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in train dataset: 7390\n"
     ]
    }
   ],
   "source": [
    "train_labels = file_label_mapping[file_label_mapping['file_name'].isin(train_face_file_names)]['person_id'].values\n",
    "print(f'Number of people in train dataset: {len(np.unique(train_labels))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YOjlY1oq74h",
    "outputId": "bb17bea0-0ecb-4b79-eb72-fe15d65d5b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0. Batch shape: torch.Size([128, 3, 160, 160])\n",
      "Batch 1. Batch shape: torch.Size([128, 3, 160, 160])\n",
      "Batch 2. Batch shape: torch.Size([15, 3, 160, 160])\n",
      "torch.Size([271, 512])\n"
     ]
    }
   ],
   "source": [
    "test_embeddings, test_face_file_names = load_data(celeba_test_dataloader, 10)\n",
    "\n",
    "# Get labels for test dataset from mapping dataframe\n",
    "test_labels = list(file_label_mapping[file_label_mapping['file_name'].isin(test_face_file_names)]['person_id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miHcqRyfHlTg"
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoM1GqEBs3IG"
   },
   "outputs": [],
   "source": [
    "def predict(test_embeddings: torch.tensor, train_embeddings: torch.tensor, file_label_mapping):\n",
    "    # Calculate distance for the test dataset and calculate accuracy\n",
    "    accuracy = 0\n",
    "    predictions = []\n",
    "    predictions_files = []\n",
    "    test_set_size = len(test_embeddings)\n",
    "    \n",
    "    for idx, test_embedding in enumerate(test_embeddings):\n",
    "        dists = [(test_embedding - e1).norm().item() for e1 in train_embeddings]\n",
    "        closest_image_file_name = face_file_names[np.argmin(dists)]\n",
    "        predicted_person_id = file_label_mapping[file_label_mapping['file_name'] == closest_image_file_name]['person_id'].values[0]\n",
    "\n",
    "        predictions.append(predicted_person_id)\n",
    "        predictions_files.append(closest_image_file_name)\n",
    "\n",
    "    return predictions, predictions_files\n",
    "\n",
    "test_predictions, test_predictions_files = predict(test_embeddings, train_embeddings, file_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EK75Ls9j2krt",
    "outputId": "dc84950c-ada1-4880-a95a-75000eb575a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f'Accuracy: {np.round(accuracy, 4)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "bbQ9uN9eiema",
    "AkkCPsvqOfj7",
    "_gXQEF5xUyly",
    "2PNpOkeQU5R1",
    "FVUpozaTlzgO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e9597514a15d9b9f9e00f05466d4e27587ef46450885503145645716a45da41a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
