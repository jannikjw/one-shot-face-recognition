{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TwXdV7BHeoT"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xpu7yWT8xlz5",
    "outputId": "809d21f8-9ae7-41f4-d617-bcf41db6e50c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uzqksv_eFu9C",
    "outputId": "06f645b1-7b42-4f76-cd9c-58352fe1fda0"
   },
   "outputs": [],
   "source": [
    "# !pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YpJrR0U9x8k7"
   },
   "outputs": [],
   "source": [
    "# requirements_file_path = \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\"\n",
    "\n",
    "# !cat \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\" | xargs -n 1 pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "qaGV91DX1SIf",
    "outputId": "41e58374-8636-4077-c5b5-15867d881dfa"
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, training, fixed_image_standardization\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import src\n",
    "from tqdm.notebook import tqdm\n",
    "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, save_file_names, CelebADatasetTriplet\n",
    "from src.utils.loss_functions import TripletLoss\n",
    "from imp import reload\n",
    "import shutil\n",
    "from torchsummary import summary\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IjCxN6Q4xkoN"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the label file after MTCNN face extraction (enable if you don't have the updated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_mapping_file = 'data/identity_CelebA_train_test_split.txt'\n",
    "# img_folder = 'data/img_align_celeba_mtcnn'\n",
    "\n",
    "# label_df = pd.read_csv(orig_mapping_file, header=None, sep=\" \", names=[\"file_name\", \"person_id\", \"is_train\"])\n",
    "\n",
    "# count=0\n",
    "# files = []\n",
    "# for filename in os.listdir(img_folder):\n",
    "#     files.append(filename)\n",
    "\n",
    "# file_names = label_df[label_df[\"file_name\"].isin(files)]\n",
    "# file_names.to_csv(\"data/identity_CelebA_train_test_split_mtcnn.txt\", sep=\" \", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ta2e8wa3UM"
   },
   "source": [
    "# Define CelebA Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hkdQB_ZODfmw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image names size is: 202292\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset\n",
    "# Path to directory with all the images\n",
    "img_folder = 'data/img_align_celeba_mtcnn'\n",
    "mapping_file = 'data/identity_CelebA_train_test_split_mtcnn.txt'\n",
    "\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 160\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADatasetTriplet(img_folder, mapping_file, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192135"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celeba_dataset.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dP2V5_HTDaMt"
   },
   "outputs": [],
   "source": [
    "## Create a dataloader\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,  # type: ignore\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the resnet model, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.5\n",
    "gamma = 0.1\n",
    "lr = 0.1\n",
    "epochs = 200\n",
    "\n",
    "schedule = [40, 80, 130, 160]\n",
    "str_schedule = \"_\".join(map(str, schedule)) #'30_50_70_80'\n",
    "\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all the layers except last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = resnet.state_dict()\n",
    "for name, param in resnet.named_parameters():\n",
    "    if param.requires_grad == False:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"last\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# params = list(model.LAST_LAYER.parameters())\n",
    "# optimizer = torch.optim.SGD(params, lr=lr)\n",
    "set_parameter_requires_grad(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_linear.weight\n",
      "last_bn.weight\n",
      "last_bn.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in resnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=lr)\n",
    "criterion = TripletLoss(margin=margin)\n",
    "\n",
    "# multistep LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedule, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 160, 160])\n",
      "torch.Size([1, 3, 160, 160])\n",
      "The distance between anchor and positive: 0.6588835716247559\n",
      "The distance between anchor and negative: 1.9983004331588745\n"
     ]
    }
   ],
   "source": [
    "resnet.eval().to(device)\n",
    "\n",
    "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
    "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
    "print(test_anchor.shape)\n",
    "\n",
    "test_anchor_emb = resnet(test_anchor[None, :].to(device))\n",
    "test_pos_emb = resnet(test_pos[None, :].to(device))\n",
    "test_neg_emb = resnet(test_neg[None, :].to(device))\n",
    "print(test_anchor[None, :].shape)\n",
    "\n",
    "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
    "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
    "\n",
    "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
    "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f833e73f59e04593a7a1088f28853b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 - Loss: 0.0551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 - Loss: 0.0382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/200 - Loss: 0.0337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/200 - Loss: 0.0332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/200 - Loss: 0.0326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/200 - Loss: 0.0332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/200 - Loss: 0.0330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/200 - Loss: 0.0316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/200 - Loss: 0.0316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd9aa3103b648079d8e34bb8f714282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet.train()\n",
    "\n",
    "loss_total = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True, position=0):\n",
    "    running_loss = []\n",
    "    for step, (anchors, positives, negatives, labels) in enumerate(tqdm(celeba_dataloader, \n",
    "                                                desc=\"Training\", position=1, leave=False)):\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        if anchors.shape[0] == 1:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_emb = resnet(anchors)\n",
    "        positive_emb = resnet(positives)\n",
    "        negative_emb = resnet(negatives)\n",
    "\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        # print(\"loss is {}\".format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        # if step > 50:\n",
    "        #     break\n",
    "        \n",
    "    loss_total.append(np.mean(running_loss))\n",
    "    learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch, epochs, np.mean(running_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing loss function\n",
    "plt.plot(loss_total)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"TripletLoss\")\n",
    "plt.title(\"Training Triplet Loss\")\n",
    "plt.savefig(f\"loss_curves/loss_curve_MTCNN_epoch{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Learning rates with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing loss function\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate\")\n",
    "plt.savefig(f\"loss_curves/learning_curve_MTCNN_epoch{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"models/facenet_model_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    torch.save(resnet, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path = f\"models/facenet_model_statedict_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pth\"\n",
    "if not os.path.exists(model_state_path):\n",
    "    torch.save(resnet.state_dict(), model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval().to(device)\n",
    "\n",
    "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
    "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
    "\n",
    "test_anchor_emb = resnet(test_anchor[None, :].to(device))\n",
    "test_pos_emb = resnet(test_pos[None, :].to(device))\n",
    "test_neg_emb = resnet(test_neg[None, :].to(device))\n",
    "\n",
    "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
    "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
    "\n",
    "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
    "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anchor_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dataset.train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "celeba_dataset.train_df.iloc[0][\"file_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vault folder and vault mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_path = \"data/oneshot_vault\"\n",
    "label_file = \"data/identity_vault_person.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vault and test label file\n",
    "if not os.path.exists(vault_path):\n",
    "    os.makedirs(vault_path)\n",
    "\n",
    "    # copying train images in the vault location and appending the label file\n",
    "    with open(label_file, \"w\") as v_file:\n",
    "        for i in range(len(celeba_dataset)):\n",
    "            file = celeba_dataset.train_df.iloc[i][\"file_name\"]\n",
    "            label = str(celeba_dataset.train_df.iloc[i][\"person_id\"])\n",
    "            v_file.write(file+\" \"+label+\"\\n\")\n",
    "\n",
    "            # copying the file to the new folder\n",
    "            src_file = os.path.join(img_folder, file)\n",
    "            dst_file = os.path.join(vault_path, file)\n",
    "            shutil.copy(src_file, dst_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vault embeddings or load vault embeddings if already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create embeddings    \n",
    "def create_embeddings(celeba_dataloader, model):\n",
    "    # initializing embedding vector and gt_label list\n",
    "    embeddings = torch.tensor([])\n",
    "    gt_labels = []\n",
    "    \n",
    "    # creating embeddings \n",
    "    for step, (anchors, positives, negatives, labels) in enumerate(tqdm(celeba_dataloader, \n",
    "                                                            desc=\"Training\", position=1)):\n",
    "        anchors = anchors.to(device)\n",
    "        img_embs = model(anchors).detach().cpu()\n",
    "        \n",
    "        embeddings = torch.cat([embeddings, img_embs])\n",
    "        gt_labels.extend(labels)\n",
    "\n",
    "    return embeddings, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_embeddings_file = f\"pytorch_objects/vault_embeddings_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pickle\"\n",
    "vault_gt_labels_file = f\"pytorch_objects/vault_gt_labels_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pickle\"\n",
    "\n",
    "# setting model to eval mode\n",
    "resnet.eval().to(device)\n",
    "\n",
    "if not os.path.exists(vault_embeddings_file) or not os.path.exists(vault_gt_labels_file):\n",
    "    embeddings, gt_labels = create_embeddings(celeba_dataloader = celeba_dataloader,\n",
    "                                              model = resnet)\n",
    "    \n",
    "    torch.save(embeddings, vault_embeddings_file)\n",
    "    torch.save(gt_labels, vault_gt_labels_file)\n",
    "else:\n",
    "    embeddings = torch.load(vault_embeddings_file)\n",
    "    gt_labels = torch.load(vault_gt_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not torch.is_tensor(gt_labels):\n",
    "#     gt_labels = torch.tensor(gt_labels).to(device)\n",
    "# else:\n",
    "#     gt_labels = gt_labels.to(device)\n",
    "# gt_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "embeddings = embeddings.detach().cpu()\n",
    "# gt_labels = gt_labels.detach().cpu()\n",
    "gt_labels = torch.tensor(gt_labels)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(embeddings, gt_labels)\n",
    "# score = knn.score(test_embeddings, test_labels)\n",
    "\n",
    "# print(f'Pre-trained model: Accuracy = {score}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# a = torch.rand(2,3)[None,:].transpose(2,1)#[None,:].transpose(2,1)\n",
    "# b = torch.rand(4,3)[:, :, None]#.transpose(2,1)\n",
    "# c=a-b\n",
    "# print(f\"a is: {a}\")\n",
    "# print(f\"b is: {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = c.pow(2).sum(axis=1).transpose(1,0)\n",
    "# torch.argmin(d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image:\n",
    "\n",
    "def calculate_label(test_images, embeddings, gt_labels, embedding_model):\n",
    "    # test_image_file = \"s1_9.pgm\"\n",
    "    test_images = test_images.to(device)\n",
    "\n",
    "    test_img_emb = embedding_model(test_images).detach().cpu()\n",
    "    test_img_emb = test_img_emb[None,:].transpose(2,1).to(device)\n",
    "\n",
    "    distance_mat = (test_img_emb - embeddings).pow(2).sum(axis=1).transpose(1,0)\n",
    "    test_label_pred = gt_labels[torch.argmin(distance_mat, axis=1)]\n",
    "\n",
    "    return test_label_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# celeba_dataset.is_train = False\n",
    "# # test_gt_labels = torch.tensor([]).type(torch.int)\n",
    "\n",
    "# test_imgs, test_labels = next(iter(celeba_dataloader))\n",
    "# print(f\"Shape of test_imgs: {test_imgs.shape}\")\n",
    "# print(f\"test labels: {test_labels}\")\n",
    "\n",
    "# # test_gt_labels = torch.cat([test_gt_labels, test_labels])\n",
    "# calculate_label(test_imgs, embeddings_, torch.tensor(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.is_tensor(gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dataset.is_train = False\n",
    "\n",
    "test_predictions = torch.tensor([]).type(torch.int)\n",
    "test_gt_labels = torch.tensor([]).type(torch.int)\n",
    "\n",
    "test_embeddings = torch.tensor([])\n",
    "\n",
    "# if not torch.is_tensor(gt_labels):\n",
    "#     gt_labels = torch.tensor(gt_labels).to(device)\n",
    "# else:\n",
    "#     gt_labels = gt_labels.to(device)\n",
    "gt_labels = gt_labels.detach().cpu()\n",
    "\n",
    "# embeddings_ = embeddings[:, :, None].to(device)\n",
    "\n",
    "for i, (test_imgs, test_labels) in enumerate(tqdm(celeba_dataloader, \n",
    "                                desc=\"Training\", position=1, leave=False)):\n",
    "    test_gt_labels = torch.cat([test_gt_labels, test_labels])\n",
    "    \n",
    "    #new\n",
    "    test_imgs = test_imgs.to(device)\n",
    "    test_embs = resnet(test_imgs).detach().cpu()\n",
    "    test_embeddings = torch.cat([test_embeddings, test_embs])\n",
    "    \n",
    "#     test_pred_labels = calculate_label(test_images = test_imgs, embeddings = embeddings_,\n",
    "#                                        gt_labels=gt_labels, embedding_model=resnet).detach().cpu()\n",
    "    \n",
    "#     test_predictions = torch.cat([test_predictions, test_pred_labels])\n",
    "    \n",
    "    if i>200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_file = f\"pytorch_objects/test_embeddings_epochs_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pickle\"\n",
    "test_gt_labels_file = f\"pytorch_objects/test_gt_labels_epochs{epochs}_margin{margin}_lr{lr}_schedule{str_schedule}.pickle\"\n",
    "\n",
    "if not os.path.exists(test_embeddings_file) or not os.path.exists(test_gt_labels_file):\n",
    "    torch.save(test_embeddings, test_embeddings_file)\n",
    "    torch.save(test_gt_labels, test_gt_labels_file)\n",
    "# test_embeddings = torch.load(test_embeddings_file)\n",
    "# test_gt_labels = torch.load(test_gt_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = knn.score(test_embeddings, test_gt_labels)\n",
    "\n",
    "print(f'trained model: Accuracy = {score}.')\n",
    "# trained model: Accuracy = 0.44198638613861385."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = test_predictions == test_gt_labels\n",
    "# accuracy = accuracy.int().sum()/len(accuracy)\n",
    "# print(f'Accuracy for the model: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gt_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9TwXdV7BHeoT",
    "k0mnB3l9Ha9D",
    "w7Ta2e8wa3UM",
    "gU46F-JyF2PA",
    "3JLywKFbopy9",
    "aEBA_FaWHpgY",
    "IJAT0dvwHfaa",
    "vqPiqddKE51v"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "one-shot-face-recognition",
   "language": "python",
   "name": "one-shot-face-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "915d08fc081d212946bc55eb28a5b744f9509c1c054f3f4f212ad3c247333ef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
