{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwXdV7BHeoT"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpu7yWT8xlz5",
        "outputId": "809d21f8-9ae7-41f4-d617-bcf41db6e50c"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzqksv_eFu9C",
        "outputId": "06f645b1-7b42-4f76-cd9c-58352fe1fda0"
      },
      "outputs": [],
      "source": [
        "# !pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YpJrR0U9x8k7"
      },
      "outputs": [],
      "source": [
        "# requirements_file_path = \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\"\n",
        "\n",
        "# !cat \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\" | xargs -n 1 pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "qaGV91DX1SIf",
        "outputId": "41e58374-8636-4077-c5b5-15867d881dfa"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile \n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "import src\n",
        "from tqdm.notebook import tqdm\n",
        "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, save_file_names, CelebADatasetTriplet\n",
        "from src.utils.loss_functions import TripletLoss\n",
        "from imp import reload\n",
        "\n",
        "workers = 0 if os.name == 'nt' else 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IjCxN6Q4xkoN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Ta2e8wa3UM"
      },
      "source": [
        "# Define CelebA Dataset and Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hkdQB_ZODfmw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image names size is: 202599\n"
          ]
        }
      ],
      "source": [
        "## Load the dataset\n",
        "# Path to directory with all the images\n",
        "img_folder = 'data/img_align_celeba'\n",
        "mapping_file = 'data/identity_CelebA_train_test_split.txt'\n",
        "\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 160\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADatasetTriplet(img_folder, mapping_file, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192422"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(celeba_dataset.test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dP2V5_HTDaMt"
      },
      "outputs": [],
      "source": [
        "## Create a dataloader\n",
        "# Batch size during training\n",
        "batch_size = 8\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "\n",
        "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,  # type: ignore\n",
        "                                                batch_size=batch_size,\n",
        "                                                num_workers=num_workers,\n",
        "                                                pin_memory=pin_memory,\n",
        "                                                shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceNet Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the resnet model, optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').to(device)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.0001)\n",
        "criterion = TripletLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The distance between anchor and positive: 0.9184950590133667\n",
            "The distance between anchor and negative: 1.6955020427703857\n"
          ]
        }
      ],
      "source": [
        "resnet.eval().to(device)\n",
        "\n",
        "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
        "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
        "\n",
        "test_anchor_emb = resnet(test_anchor[None, :])\n",
        "test_pos_emb = resnet(test_pos[None, :])\n",
        "test_neg_emb = resnet(test_neg[None, :])\n",
        "\n",
        "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
        "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
        "\n",
        "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
        "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "147a4539434d44b1bd7fa216df51aa5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|                                             | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f75e7f1d56f84eb99d5059eab8f87640",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92@gmail.com/My Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# print(\"loss is {}\".format(loss))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     running_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one_shot_learning_triplet_loss.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss_total\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(running_loss))\n",
            "File \u001b[0;32m~/miniforge3/envs/torch-env/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/miniforge3/envs/torch-env/lib/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[0;32m~/miniforge3/envs/torch-env/lib/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/miniforge3/envs/torch-env/lib/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[0;32m~/miniforge3/envs/torch-env/lib/python3.9/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    412\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "resnet.train()\n",
        "epochs = 5\n",
        "loss_total = []\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True, ncols=80, position=0):\n",
        "    running_loss = []\n",
        "    for step, (anchors, positives, negatives, labels) in enumerate(tqdm(celeba_dataloader, desc=\"Training\", ncols=80, position=1, leave=False)):\n",
        "        anchors = anchors.to(device)\n",
        "        positives = positives.to(device)\n",
        "        negatives = negatives.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        anchor_emb = resnet(anchors)\n",
        "        positive_emb = resnet(positives)\n",
        "        negative_emb = resnet(negatives)\n",
        "\n",
        "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
        "        # print(\"loss is {}\".format(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "        \n",
        "    loss_total.append(np.mean(running_loss))\n",
        "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch, epochs, np.mean(running_loss)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# printing loss function\n",
        "plt.plot(loss_total)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"TripletLoss\")\n",
        "plt.title(\"Training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The distance between anchor and positive: 0.939580500125885\n",
            "The distance between anchor and negative: 0.6248536705970764\n"
          ]
        }
      ],
      "source": [
        "resnet.eval().to(device)\n",
        "\n",
        "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
        "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
        "\n",
        "test_anchor_emb = resnet(test_anchor[None, :])\n",
        "test_pos_emb = resnet(test_pos[None, :])\n",
        "test_neg_emb = resnet(test_neg[None, :])\n",
        "\n",
        "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
        "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
        "\n",
        "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
        "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_anchor_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "vault_path = \"../data/oneshot_vault\"\n",
        "label_file = \"../data/identity_vault_person.txt\"\n",
        "\n",
        "def load_image(path, transform):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if transform:\n",
        "            img = transform(img)\n",
        "    return img\n",
        "    \n",
        "def create_embeddings(folder, label_file, model, transform):\n",
        "    label_file_dict = {}\n",
        "    gt_labels = []\n",
        "    with open(label_file, 'r') as r_file:\n",
        "        for file in r_file:\n",
        "            file = file.strip(\"\\n\").split(\" \")\n",
        "            if file[0] not in label_file_dict:\n",
        "                label_file_dict[file[0]] = file[1]\n",
        "\n",
        "    embeddings = torch.empty(len(label_file_dict), 512)\n",
        "    for i, file in enumerate(label_file_dict.keys()):\n",
        "        img = load_image(os.path.join(folder, file), transform)\n",
        "\n",
        "        img_emb = model(img[None, :])\n",
        "\n",
        "        embeddings[i] = img_emb\n",
        "        gt_labels.append(label_file_dict[file])\n",
        "\n",
        "    return embeddings, gt_labels\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "embeddings, gt_labels = create_embeddings(folder=vault_path, label_file=label_file, \n",
        "                        model=resnet, transform=transform)\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test image:\n",
        "def calculate_label(test_image_file, img_folder, transform, embeddings):\n",
        "# test_image_file = \"s1_9.pgm\"\n",
        "    test_file_path = os.path.join(img_folder, test_image_file)\n",
        "    test_img = load_image(test_file_path, transform=transform)\n",
        "\n",
        "\n",
        "    test_img_emb = resnet(test_img[None, :])\n",
        "    test_img_emb = torch.squeeze(test_img_emb, 0)\n",
        "    # print(f'Shape of test: {test_img_emb.shape}')\n",
        "    # print(f'Shape of embeddings: {embeddings.shape}')\n",
        "\n",
        "    distance_mat = (test_img_emb - embeddings).pow(2).sum(axis=1)\n",
        "    # print(distance_mat)\n",
        "    test_label_pred = gt_labels[torch.argmin(distance_mat.abs())]\n",
        "    # print(f'Ground truth label: {test_image_file.split(\"_\")[0][1]}')\n",
        "    # print(f'Predicted label: {test_label_pred}')\n",
        "\n",
        "    return int(test_label_pred)\n",
        "\n",
        "# testing on one test image\n",
        "test_image_file = \"s4_9.pgm\"\n",
        "calculate_label(test_image_file, img_folder, transform, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for the model: 0.6399999856948853\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(mapping_file, 'r') as test_labels_file:\n",
        "    test_labels = {}\n",
        "    for file in test_labels_file:\n",
        "        file = file.strip(\"\\n\").split(\" \")\n",
        "        test_labels[file[0]] = file[1]\n",
        "\n",
        "test_predictions = []\n",
        "test_gt_labels = []\n",
        "\n",
        "for i, (file, label) in enumerate(test_labels.items()):\n",
        "    test_gt_labels.append(int(label))\n",
        "\n",
        "    test_label_pred = calculate_label(file, img_folder, transform, embeddings)\n",
        "    test_predictions.append(test_label_pred)\n",
        "\n",
        "accuracy = torch.tensor(test_predictions) == torch.tensor(test_gt_labels)\n",
        "accuracy = accuracy.int().sum()/len(accuracy)\n",
        "print(f'Accuracy for the model: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9TwXdV7BHeoT",
        "k0mnB3l9Ha9D",
        "w7Ta2e8wa3UM",
        "gU46F-JyF2PA",
        "3JLywKFbopy9",
        "aEBA_FaWHpgY",
        "IJAT0dvwHfaa",
        "vqPiqddKE51v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('torch-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "915d08fc081d212946bc55eb28a5b744f9509c1c054f3f4f212ad3c247333ef8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
