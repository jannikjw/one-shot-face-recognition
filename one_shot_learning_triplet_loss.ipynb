{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwXdV7BHeoT"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpu7yWT8xlz5",
        "outputId": "809d21f8-9ae7-41f4-d617-bcf41db6e50c"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzqksv_eFu9C",
        "outputId": "06f645b1-7b42-4f76-cd9c-58352fe1fda0"
      },
      "outputs": [],
      "source": [
        "# !pip install facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YpJrR0U9x8k7"
      },
      "outputs": [],
      "source": [
        "# requirements_file_path = \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\"\n",
        "\n",
        "# !cat \"/content/drive/MyDrive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/requirements.txt\" | xargs -n 1 pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "qaGV91DX1SIf",
        "outputId": "41e58374-8636-4077-c5b5-15867d881dfa"
      },
      "outputs": [],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile \n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "import src\n",
        "from tqdm.notebook import tqdm\n",
        "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, save_file_names, CelebADatasetTriplet\n",
        "from src.utils.loss_functions import TripletLoss\n",
        "from imp import reload\n",
        "\n",
        "workers = 0 if os.name == 'nt' else 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IjCxN6Q4xkoN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Ta2e8wa3UM"
      },
      "source": [
        "# Define CelebA Dataset and Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hkdQB_ZODfmw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image names size is: 202599\n"
          ]
        }
      ],
      "source": [
        "## Load the dataset\n",
        "# Path to directory with all the images\n",
        "img_folder = 'data/img_align_celeba'\n",
        "mapping_file = 'data/identity_CelebA_train_test_split.txt'\n",
        "\n",
        "# Spatial size of training images, images are resized to this size.\n",
        "image_size = 160\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the dataset from file and apply transformations\n",
        "celeba_dataset = CelebADatasetTriplet(img_folder, mapping_file, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "192422"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(celeba_dataset.test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dP2V5_HTDaMt"
      },
      "outputs": [],
      "source": [
        "## Create a dataloader\n",
        "# Batch size during training\n",
        "batch_size = 8\n",
        "# Number of workers for the dataloader\n",
        "num_workers = 0 if device.type == 'cuda' else 2\n",
        "# Whether to put fetched data tensors to pinned memory\n",
        "pin_memory = True if device.type == 'cuda' else False\n",
        "\n",
        "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,  # type: ignore\n",
        "                                                batch_size=batch_size,\n",
        "                                                num_workers=num_workers,\n",
        "                                                pin_memory=pin_memory,\n",
        "                                                shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FaceNet Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the resnet model, optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').to(device)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.0001)\n",
        "criterion = TripletLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The distance between anchor and positive: 0.9184950590133667\n",
            "The distance between anchor and negative: 1.6955020427703857\n"
          ]
        }
      ],
      "source": [
        "resnet.eval().to(device)\n",
        "\n",
        "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
        "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
        "\n",
        "test_anchor_emb = resnet(test_anchor[None, :])\n",
        "test_pos_emb = resnet(test_pos[None, :])\n",
        "test_neg_emb = resnet(test_neg[None, :])\n",
        "\n",
        "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
        "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
        "\n",
        "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
        "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff789a45a7774f2e9057131e5c5045b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|                                             | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d62f584794642b1b03380a22755366d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/5 - Loss: 0.2107\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f19df46d65a143d0aef33f063f2c15b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/5 - Loss: 0.0313\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2ed427288bd4f2294ddbddb02be1a18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2/5 - Loss: 0.0125\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26e54adcea734386a72328bec7fc094f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3/5 - Loss: 0.0051\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0ce3d1124db45c49479028f111aeb80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|                                        | 0/1273 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4/5 - Loss: 0.0025\n"
          ]
        }
      ],
      "source": [
        "resnet.train()\n",
        "epochs = 5\n",
        "loss_total = []\n",
        "\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True, ncols=80, position=0):\n",
        "    running_loss = []\n",
        "    for step, (anchors, positives, negatives, labels) in enumerate(tqdm(celeba_dataloader, desc=\"Training\", ncols=80, position=1, leave=False)):\n",
        "        anchors = anchors.to(device)\n",
        "        positives = positives.to(device)\n",
        "        negatives = negatives.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        anchor_emb = resnet(anchors)\n",
        "        positive_emb = resnet(positives)\n",
        "        negative_emb = resnet(negatives)\n",
        "\n",
        "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
        "        # print(\"loss is {}\".format(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss.append(loss.cpu().detach().numpy())\n",
        "        # if step > 50:\n",
        "        #     break\n",
        "        \n",
        "    loss_total.append(np.mean(running_loss))\n",
        "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch, epochs, np.mean(running_loss)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# printing loss function\n",
        "plt.plot(loss_total)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"TripletLoss\")\n",
        "plt.title(\"Training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The distance between anchor and positive: 1.1327433586120605\n",
            "The distance between anchor and negative: 2.6701910495758057\n"
          ]
        }
      ],
      "source": [
        "resnet.eval().to(device)\n",
        "\n",
        "test_anchor, test_pos, test_neg, anchor_label = celeba_dataset[1]\n",
        "# test_anchor, test_pos, test_neg, anchor_label = test_anchor[1], test_pos[1], test_neg[1], anchor_label[1]\n",
        "\n",
        "test_anchor_emb = resnet(test_anchor[None, :])\n",
        "test_pos_emb = resnet(test_pos[None, :])\n",
        "test_neg_emb = resnet(test_neg[None, :])\n",
        "\n",
        "pos_dist = criterion.cal_distance(test_anchor_emb, test_pos_emb)\n",
        "neg_dist = criterion.cal_distance(test_anchor_emb, test_neg_emb)\n",
        "\n",
        "print(\"The distance between anchor and positive: {}\".format(pos_dist[0]))\n",
        "print(\"The distance between anchor and negative: {}\".format(neg_dist[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_anchor_emb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "vault_path = \"data/oneshot_vault\"\n",
        "label_file = \"data/identity_vault_person.txt\"\n",
        "\n",
        "def load_image(path, transform):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    if transform:\n",
        "            img = transform(img)\n",
        "    return img\n",
        "    \n",
        "def create_embeddings(folder, label_file, model, transform):\n",
        "    label_file_dict = {}\n",
        "    gt_labels = []\n",
        "    with open(label_file, 'r') as r_file:\n",
        "        for file in r_file:\n",
        "            file = file.strip(\"\\n\").split(\" \")\n",
        "            if file[0] not in label_file_dict:\n",
        "                label_file_dict[file[0]] = file[1]\n",
        "\n",
        "    embeddings = torch.empty(len(label_file_dict), 512)\n",
        "    for i, file in enumerate(label_file_dict.keys()):\n",
        "        img = load_image(os.path.join(folder, file), transform)\n",
        "\n",
        "        img_emb = model(img[None, :])\n",
        "\n",
        "        embeddings[i] = img_emb\n",
        "        gt_labels.append(label_file_dict[file])\n",
        "\n",
        "    return embeddings, gt_labels\n",
        "\n",
        "\n",
        "resnet.eval().to(device)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "embeddings, gt_labels = create_embeddings(folder=vault_path, label_file=label_file, \n",
        "                        model=resnet, transform=transform)\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4905"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test image:\n",
        "def calculate_label(test_image_file, img_folder, transform, embeddings):\n",
        "# test_image_file = \"s1_9.pgm\"\n",
        "    test_file_path = os.path.join(img_folder, test_image_file)\n",
        "    test_img = load_image(test_file_path, transform=transform)\n",
        "\n",
        "\n",
        "    test_img_emb = resnet(test_img[None, :])\n",
        "    test_img_emb = torch.squeeze(test_img_emb, 0)\n",
        "    # print(f'Shape of test: {test_img_emb.shape}')\n",
        "    # print(f'Shape of embeddings: {embeddings.shape}')\n",
        "\n",
        "    distance_mat = (test_img_emb - embeddings).pow(2).sum(axis=1)\n",
        "    # print(distance_mat)\n",
        "    test_label_pred = gt_labels[torch.argmin(distance_mat.abs())]\n",
        "    # print(f'Ground truth label: {test_image_file.split(\"_\")[0][1]}')\n",
        "    # print(f'Predicted label: {test_label_pred}')\n",
        "\n",
        "    return int(test_label_pred)\n",
        "\n",
        "# testing on one test image\n",
        "test_image_file = \"000032.jpg\"\n",
        "calculate_label(test_image_file, img_folder, transform, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for the model: 0.06622516363859177\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(mapping_file, 'r') as test_labels_file:\n",
        "    test_labels = {}\n",
        "    for i, file in enumerate(test_labels_file):\n",
        "        file = file.strip(\"\\n\").split(\" \")\n",
        "        test_labels[file[0]] = file[1]\n",
        "        # if i > 300:\n",
        "        #     break\n",
        "\n",
        "test_predictions = []\n",
        "test_gt_labels = []\n",
        "\n",
        "for i, (file, label) in enumerate(test_labels.items()):\n",
        "    test_gt_labels.append(int(label))\n",
        "\n",
        "    test_label_pred = calculate_label(file, img_folder, transform, embeddings)\n",
        "    test_predictions.append(test_label_pred)\n",
        "\n",
        "accuracy = torch.tensor(test_predictions) == torch.tensor(test_gt_labels)\n",
        "accuracy = accuracy.int().sum()/len(accuracy)\n",
        "print(f'Accuracy for the model: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9TwXdV7BHeoT",
        "k0mnB3l9Ha9D",
        "w7Ta2e8wa3UM",
        "gU46F-JyF2PA",
        "3JLywKFbopy9",
        "aEBA_FaWHpgY",
        "IJAT0dvwHfaa",
        "vqPiqddKE51v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('torch-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "915d08fc081d212946bc55eb28a5b744f9509c1c054f3f4f212ad3c247333ef8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
