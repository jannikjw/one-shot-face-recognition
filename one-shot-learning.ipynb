{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TwXdV7BHeoT"
   },
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaGV91DX1SIf"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel tensorflow (Python 3.9.7) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import src\n",
    "from src.utils.celeba_helper import CelebADataset, CelebAClassifier, save_file_names\n",
    "from imp import reload\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ta2e8wa3UM"
   },
   "source": [
    "# Define CelebA Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hkdQB_ZODfmw"
   },
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "# Path to directory with all the images\n",
    "img_folder = '../data/img_align_celeba'\n",
    "mapping_file = '../data/identity_CelebA.txt'\n",
    "\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 160\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADataset(img_folder, mapping_file, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dP2V5_HTDaMt"
   },
   "outputs": [],
   "source": [
    "## Create a dataloader\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "celeba_dataloader = torch.utils.data.DataLoader(celeba_dataset,  # type: ignore\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU46F-JyF2PA"
   },
   "source": [
    "# Setup FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-4xtVzckHtgP"
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=image_size, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, keep_all=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eoM1GqEBs3IG"
   },
   "outputs": [],
   "source": [
    "classifier = CelebAClassifier(celeba_dataloader, detection_model=mtcnn, embedding_model=resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "888SZ_q2D6um"
   },
   "source": [
    "# One-Shot Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBJRHcdjEAWa"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CYXmn3XQtJAm"
   },
   "outputs": [],
   "source": [
    "# Select train files\n",
    "file_label_mapping = celeba_dataset.get_file_label_mapping()\n",
    "first_file_for_each_person_df = file_label_mapping.sort_values(by='person_id').groupby('person_id').agg(['min', 'count'])\n",
    "train_files = np.sort(first_file_for_each_person_df[first_file_for_each_person_df['file_name']['count'] > 1]['file_name']['min'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "juEIVivpnn5T",
    "outputId": "b9953e1f-df4b-4a21-929e-ce3ee40585e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "/Users/vspvikram/miniforge3/envs/tensorflow/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/Users/vspvikram/miniforge3/envs/tensorflow/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92@gmail.com/My Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m file_names_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpytorch_objects/train_file_names_all_1img\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(embeddings_path) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_names_path):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_embeddings, train_face_file_names \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mload_data_specific_images(files_to_load\u001b[39m=\u001b[39;49mtrain_files)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(train_embeddings, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpytorch_objects/train_embeddings_all_1img.pickle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vspvikram/Library/CloudStorage/GoogleDrive-kingvsp92%40gmail.com/My%20Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/one-shot-learning.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     save_file_names(train_face_file_names, \u001b[39m'\u001b[39m\u001b[39mpytorch_objects/train_file_names_all_1img\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-kingvsp92@gmail.com/My Drive/Masters/Prep/Projects/OneShot_JPMC/one-shot-face-recognition/src/utils/celeba_helper.py:127\u001b[0m, in \u001b[0;36mCelebAClassifier.load_data_specific_images\u001b[0;34m(self, files_to_load)\u001b[0m\n\u001b[1;32m    125\u001b[0m count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    126\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mroot_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m aligned, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetection_model(img, return_prob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m aligned \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     aligned \u001b[39m=\u001b[39m aligned\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    131\u001b[0m         [\u001b[39m1\u001b[39m, aligned\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], aligned\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], aligned\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]]\n\u001b[1;32m    132\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m batch_boxes, batch_probs, batch_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(img, landmarks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Select faces\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_all:\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/facenet_pytorch/models/utils/detect_face.py:79\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m boxes\u001b[39m.\u001b[39mappend(boxes_scale)\n\u001b[1;32m     77\u001b[0m image_inds\u001b[39m.\u001b[39mappend(image_inds_scale)\n\u001b[0;32m---> 79\u001b[0m pick \u001b[39m=\u001b[39m batched_nms(boxes_scale[:, :\u001b[39m4\u001b[39;49m], boxes_scale[:, \u001b[39m4\u001b[39;49m], image_inds_scale, \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     80\u001b[0m scale_picks\u001b[39m.\u001b[39mappend(pick \u001b[39m+\u001b[39m offset)\n\u001b[1;32m     81\u001b[0m offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m boxes_scale\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torchvision/ops/boxes.py:68\u001b[0m, in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/jit/_trace.py:1118\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m   1115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   1117\u001b[0m         \u001b[39m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> 1118\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1120\u001b[0m     compiled_fn \u001b[39m=\u001b[39m script(wrapper\u001b[39m.\u001b[39m__original_fn)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m compiled_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torchvision/ops/boxes.py:87\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     85\u001b[0m offsets \u001b[39m=\u001b[39m idxs\u001b[39m.\u001b[39mto(boxes) \u001b[39m*\u001b[39m (max_coordinate \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(boxes))\n\u001b[1;32m     86\u001b[0m boxes_for_nms \u001b[39m=\u001b[39m boxes \u001b[39m+\u001b[39m offsets[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> 87\u001b[0m keep \u001b[39m=\u001b[39m nms(boxes_for_nms, scores, iou_threshold)\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m keep\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torchvision/ops/boxes.py:34\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnms\u001b[39m(boxes: Tensor, scores: Tensor, iou_threshold: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    Performs non-maximum suppression (NMS) on the boxes according\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    to their intersection-over-union (IoU).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m        by NMS, sorted in decreasing order of scores\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     _assert_has_ops()\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torchvision/extension.py:62\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_has_ops\u001b[39m():\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_ops():\n\u001b[0;32m---> 62\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "# The following takes a long time, as one image is loaded and embedded for each person. Run this once for the desired dataset. \n",
    "# Afterwards, load the data from the pickle file.\n",
    "\n",
    "if not os.path.exists('pytorch_objects'):\n",
    "  os.makedirs('pytorch_objects')\n",
    "\n",
    "embeddings_path = 'pytorch_objects/train_embeddings_all_1img.pickle'\n",
    "file_names_path = 'pytorch_objects/train_file_names_all_1img'\n",
    "\n",
    "if not os.path.exists(embeddings_path) or not os.path.exists(file_names_path):\n",
    "    train_embeddings, train_face_file_names = classifier.load_data_specific_images(files_to_load=train_files)\n",
    "    torch.save(train_embeddings, f'pytorch_objects/train_embeddings_all_1img.pickle')\n",
    "    save_file_names(train_face_file_names, 'pytorch_objects/train_file_names_all_1img')\n",
    "else:\n",
    "    train_embeddings = torch.load(embeddings_path)\n",
    "    train_face_file_names = []\n",
    "    with open(file_names_path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            x = line[:-1]\n",
    "            # add current item to the list\n",
    "            train_face_file_names.append(x)\n",
    "\n",
    "print(f'Size of train dataset: {train_embeddings.shape}')\n",
    "train_labels = celeba_dataset.get_labels_from_file_names(train_face_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "v1YUbSyeFiwH"
   },
   "outputs": [],
   "source": [
    "# Select test files\n",
    "second_file_for_each_person_df = file_label_mapping[~file_label_mapping['file_name'].isin(first_file_for_each_person_df['file_name']['min'])].sort_values(by='person_id').groupby('person_id').agg(['min', 'count'])\n",
    "test_files = np.sort(second_file_for_each_person_df[second_file_for_each_person_df['file_name']['count'] >= 1]['file_name']['min'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "n6Znlo91Fzl6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: torch.Size([10117, 512])\n",
      "Number of people in dataset: 10117\n"
     ]
    }
   ],
   "source": [
    "# The following takes a long time, as one image is loaded and embedded for each person. Run this once for the desired dataset. \n",
    "# Afterwards, load the data from the pickle file.\n",
    "\n",
    "embeddings_path = 'pytorch_objects/test_embeddings_all_1img.pickle'\n",
    "file_names_path = 'pytorch_objects/test_file_names_all_1img'\n",
    "\n",
    "if not os.path.exists(embeddings_path) or not os.path.exists(file_names_path):\n",
    "    test_embeddings, test_face_file_names = classifier.load_data_specific_images(files_to_load=test_files)\n",
    "    torch.save(test_embeddings, f'pytorch_objects/test_embeddings_all_1img.pickle')\n",
    "    save_file_names(test_face_file_names, 'pytorch_objects/test_file_names_all_1img')\n",
    "else:\n",
    "    test_embeddings = torch.load(embeddings_path)\n",
    "    test_face_file_names = []\n",
    "    with open(file_names_path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            x = line[:-1]\n",
    "            # add current item to the list\n",
    "            test_face_file_names.append(x)\n",
    "            \n",
    "print(f'Size of test dataset: {test_embeddings.shape}')\n",
    "test_labels = celeba_dataset.get_labels_from_file_names(test_face_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gfa56954O2uM"
   },
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells, each might take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OzGjQxGUO_bt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the norm_2 metric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10117it [24:21,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - norm_2: 0.6955\n",
      "Calculating the norm_2_squared metric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10117it [23:22,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - norm_2_squared: 0.6955\n",
      "Calculating the cosine_similarity metric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10117it [53:05,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - cosine_similarity: 0.6955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for similarity_metric in ['norm_2', 'norm_2_squared', 'cosine_similarity']:\n",
    "    test_predictions, test_predictions_files = classifier.predict(test_embeddings, train_embeddings, train_face_file_names, similarity_metric)\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f'Accuracy - {similarity_metric}: {np.round(accuracy, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yoJ75E_aQ0T2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', verbose=True)\n",
    "model.fit(train_embeddings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAuqoUVAQ0T4"
   },
   "outputs": [],
   "source": [
    "# train_predictions = model.predict(train_embeddings)\n",
    "test_predictions = model.predict(test_embeddings)\n",
    "# train_predictions = model.predict(train_embeddings)\n",
    "\n",
    "# score_train = accuracy_score(train_labels, train_predictions)\n",
    "score_test = accuracy_score(test_labels, test_predictions)\n",
    "# score_train = accuracy_score(train_labels, train_predictions)\n",
    "\n",
    "# print(f'Accuracy: train = {np.round(score_train*100, 3)}%')\n",
    "print(f'Accuracy: test = {np.round(score_test*100, 3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(train_embeddings[0:1000], train_labels[0:1000])\n",
    "\n",
    "knn.score(test_embeddings[0:1000], test_labels[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9TwXdV7BHeoT",
    "k0mnB3l9Ha9D",
    "w7Ta2e8wa3UM",
    "gU46F-JyF2PA",
    "3JLywKFbopy9",
    "aEBA_FaWHpgY",
    "IJAT0dvwHfaa",
    "vqPiqddKE51v"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "915d08fc081d212946bc55eb28a5b744f9509c1c054f3f4f212ad3c247333ef8"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4f85a6f0f9324ea486123de2cbe30e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b34a69a6ae5b4d9cba0c3122347a71a9",
      "max": 111898327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2ee6bf5a22c4212b8a1561806fd4e3d",
      "value": 111898327
     }
    },
    "59bde1c8a1b64a49ba27815343158bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c9fca3e08b64557bc1643b702584a3e",
       "IPY_MODEL_4f85a6f0f9324ea486123de2cbe30e8c",
       "IPY_MODEL_ac1c0e176aee4fc8b9aa118fdf969b62"
      ],
      "layout": "IPY_MODEL_82f4c9540ef54adf89c4df3a2498f5b0"
     }
    },
    "5c9fca3e08b64557bc1643b702584a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f03cdbcddb34eb7a802efa105484b73",
      "placeholder": "​",
      "style": "IPY_MODEL_f300fc24f7dc4156a362b9910af4d1fc",
      "value": "100%"
     }
    },
    "6de3e0fc1f76432f9544c2ae5c0766f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f03cdbcddb34eb7a802efa105484b73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82f4c9540ef54adf89c4df3a2498f5b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac1c0e176aee4fc8b9aa118fdf969b62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6de3e0fc1f76432f9544c2ae5c0766f0",
      "placeholder": "​",
      "style": "IPY_MODEL_d3f21c1443ba4ec1ad64a5d143598a19",
      "value": " 107M/107M [00:00&lt;00:00, 130MB/s]"
     }
    },
    "b34a69a6ae5b4d9cba0c3122347a71a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2ee6bf5a22c4212b8a1561806fd4e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3f21c1443ba4ec1ad64a5d143598a19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f300fc24f7dc4156a362b9910af4d1fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
