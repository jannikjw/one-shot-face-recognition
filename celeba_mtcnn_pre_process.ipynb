{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import src\n",
    "from src.utils.celeba_helper import CelebA_MTCNN_Helper\n",
    "from imp import reload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# celeba images are originally 178 x 218\n",
    "img_folder = 'data/img_align_celeba' \n",
    "mapping_file = 'data/identity_CelebA.txt'\n",
    "\n",
    "# define original celeba dataset before MTCNN - resize to larger 512x512 before MTCNN so it has a better chance to detect the face\n",
    "celeba_dataset = CelebA_MTCNN_Helper(img_folder, mapping_file, transform=transforms.Resize((512, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MTCNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of MTCNN should be 160x160 because that is what the FaceNet InceptionResNet is trained on with the VGGFace2 Dataset - pretrained weights\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, keep_all=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfom MTCNN facial detection\n",
    "Iterate through the DataLoader object and obtain cropped faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only Run this Code Once to Create MTCNN Cropped/Detected CelebA Image Directory**\n",
    "\n",
    "**MTCNN CelebA Dataset Saved In: 'data/img_align_celeba_mtcnn' Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new folder where the post_mtcnn celeba images will be stored\n",
    "mtcnn_img_folder = img_folder + '_mtcnn'\n",
    "mtcnn_img_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataloader\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False\n",
    "\n",
    "celeba_dataloader = DataLoader(celeba_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               num_workers=num_workers,\n",
    "                               pin_memory=pin_memory,\n",
    "                               collate_fn=training.collate_pil, # formats PIL batch correctly\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MTCNN on the original CelebA Dataset\n",
    "for i, (x, y) in enumerate(celeba_dataloader):\n",
    "    # the getitem method returns (x = [PIL img1, PIL img2, ...,], y = [img1_name, img2_name, ... ,]) for each batch in DataLoader but\n",
    "    # need to replace each img_name with the new img file path to be saved post MTCNN\n",
    "    y = [os.path.join(mtcnn_img_folder, img_file_name).replace('\\\\','/') for img_file_name in y]\n",
    "    \n",
    "    mtcnn(x, save_path=y)\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(celeba_dataloader)), end='')\n",
    "\n",
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('one-shot-face-recognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f833007afcffe21c80cae9ab338ea0326c55dd31c5b96c817689339d3442c56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
